[Data]
data_dir = expdata/dataset
target_train_file = %(data_dir)s/result_orl_ch_train.txt
target_dev_file = %(data_dir)s/result_orl_ch_val.txt
target_test_file = %(data_dir)s/result_orl_ch_test.txt
source_train_file = %(data_dir)s/result_orl_en_train.txt
min_occur_count = 0

[Save]
save_dir = expdata/dataset_output_zh
config_file = %(save_dir)s/config.cfg
save_model_path = %(save_dir)s/model
save_vocab_path = %(save_dir)s/vocab
load_dir = expdata/dataset_output_zh
load_model_path = %(load_dir)s/model
load_vocab_path = %(load_dir)s/vocab

[bert]
bert_path = bert-base-multilingual-cased
bert_config_path = %(bert_path)s/bert_config.json
bert_hidden_size = 768
output_hidden_states = True
output_attentions = False
tune_start_layer = 12

[AdapterPGN]
use_adapter = false
use_language_emb = true
one_hot = false
language_emb_size = 32
language_emb_dropout = 0.1
language_drop_rate = 0.2
num_language_features = 289
nl_project = 289
language_features = syntax_knn+phonology_knn+inventory_knn
adapter_initializer_range = 0.0001
in_langs = en, zh
out_langs = 
letter_codes = data/letter_codes.json

[Network]
model = BiLSTMCRFModel
lstm_layers = 3
word_dims = 100
predict_dims = 100
dropout_emb = 0.0
lstm_hiddens = 200
dropout_lstm_input = 0.33
dropout_lstm_hidden = 0.33
hidden_dims = 100
inner_hidden_dims = 400
number_heads = 8
num_layers = 6
dropout_hidden = 0.33

[Optimizer]
learning_rate = 2e-3
decay = .75
decay_steps = 5000
beta_1 = .9
beta_2 = .9
epsilon = 1e-12
clip = 1.0
parser_tune = 1

[Run]
train_epochs = 40
train_batch_size = 32
test_batch_size = 32
validate_every = 300
update_every = 300
save_after = 2

