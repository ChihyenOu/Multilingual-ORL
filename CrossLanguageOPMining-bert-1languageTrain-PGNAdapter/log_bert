Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
GPU available:  True
CuDNN: 
 True
Loaded config file sucessfully.
data_dir expdata/example_data
train_file expdata/example_data/result_orl_ch_train.txt
dev_file expdata/example_data/result_orl_ch_val.txt
test_file expdata/example_data/result_orl_ch_test.txt
min_occur_count 0
save_dir expdata/example_output_ch
config_file expdata/example_output_ch/config.cfg
save_model_path expdata/example_output_ch/model
save_vocab_path expdata/example_output_ch/vocab
load_dir expdata/example_output_ch
load_model_path expdata/example_output_ch/model
load_vocab_path expdata/example_output_ch/vocab
bert_path bert-base-multilingual-cased-chORL
bert_config_path bert-base-multilingual-cased-chORL/bert_config.json
bert_hidden_size 768
output_hidden_states True
output_attentions False
tune_start_layer 12
use_adapter true
use_language_emb true
num_adapters 1
adapter_size 256
one_hot false
language_emb_size 32
language_emb_dropout 0.1
language_drop_rate 0.2
num_language_features 289
nl_project 289
language_features syntax_knn+phonology_knn+inventory_knn
adapter_initializer_range 0.0001
in_langs zh
out_langs zh
letter_codes data/letter_codes.json
model BiLSTMCRFModel
lstm_layers 3
word_dims 100
predict_dims 100
dropout_emb 0.0
lstm_hiddens 200
dropout_lstm_input 0.33
dropout_lstm_hidden 0.33
hidden_dims 100
inner_hidden_dims 400
number_heads 8
num_layers 6
dropout_hidden 0.33
learning_rate 2e-3
decay .75
decay_steps 5000
beta_1 .9
beta_2 .9
epsilon 1e-12
clip 1.0
parser_tune 1
train_iters 5
train_batch_size 32
test_batch_size 32
validate_every 5
update_every 160
save_after 2
Total num:  180
Vocab info: #words 1317, #labels 8

GPU using status:  False
200 1068
Orthogonal pretrainer loss: 7.28e-30
200 600
Orthogonal pretrainer loss: 1.59e-30
200 600
Orthogonal pretrainer loss: 1.41e-30
Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Load bert vocabulary finished
Total num:  180
Total num:  21
Total num:  187
Finish code test!
