[Data]
pretrained_embeddings_file = expdata/wiki.en.vec.orl.filted.1
data_dir = expdata/opinion
target_train_file = %(data_dir)s/emnlp20orl.en.sample
target_dev_file = %(data_dir)s/emnlp20orl.ch.sample
target_test_file = %(data_dir)s/emnlp20orl.ch.sample
source_train_file = %(data_dir)s/emnlp20orl.ch.sample
min_occur_count = 0

[Save]
save_dir = expdata/opmodel
config_file = %(save_dir)s/config.cfg
save_model_path = %(save_dir)s/model
save_vocab_path = %(save_dir)s/vocab
load_dir = expdata/opmodel
load_model_path = %(load_dir)s/model
load_vocab_path = %(load_dir)s/vocab

[bert]
bert_path = bert-base-multilingual-cased
bert_config_path = bert-base-multilingual-cased/bert_config.json
bert_hidden_size = 768
; bert_path = xlm-mlm-100-1280
output_hidden_states = True
output_attentions = False
tune_start_layer = 12

[AdapterPGN]
use_adapter = true
use_language_emb = false
num_adapters = 1
adapter_size = 256
one_hot = false
language_emb_size = 32
language_emb_dropout = 0.1
language_drop_rate = 0.2
num_language_features = 289
nl_project = 289
language_features = syntax_knn+phonology_knn+inventory_knn
adapter_initializer_range = 0.0001
in_langs = en, zh
out_langs =
letter_codes = data/letter_codes.json

[Network]
model = BiLSTMCRFModel
lstm_layers = 3
word_dims = 100
predict_dims = 100
dropout_emb = 0.0
lstm_hiddens = 200
dropout_lstm_input = 0.33
dropout_lstm_hidden = 0.33
hidden_dims = 100
inner_hidden_dims = 400
number_heads = 8
num_layers = 6
dropout_hidden = 0.33

[Optimizer]
learning_rate = 2e-3
decay = .75
decay_steps = 5000
beta_1 = .9
beta_2 = .9
epsilon = 1e-12
clip = 1.0
parser_tune = 1

[Run]
train_iters = 50000
train_batch_size = 32
test_batch_size = 32
validate_every = 5
update_every = 160
save_after = 2
