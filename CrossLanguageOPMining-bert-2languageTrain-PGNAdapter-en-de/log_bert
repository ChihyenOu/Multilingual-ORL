Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.
GPU available:  True
CuDNN: 
 True
Loaded config file sucessfully.
data_dir expdata/dataset
target_train_file expdata/dataset/result_orl_de_train.txt
target_dev_file expdata/dataset/result_orl_de_val.txt
target_test_file expdata/dataset/result_orl_de_test.txt
source_train_file expdata/dataset/result_orl_en_train.txt
min_occur_count 0
save_dir expdata/dataset_output_de
config_file expdata/dataset_output_de/config.cfg
save_model_path expdata/dataset_output_de/model
save_vocab_path expdata/dataset_output_de/vocab
load_dir expdata/dataset_output_de
load_model_path expdata/dataset_output_de/model
load_vocab_path expdata/dataset_output_de/vocab
bert_path bert-base-multilingual-cased
bert_config_path bert-base-multilingual-cased/bert_config.json
bert_hidden_size 768
output_hidden_states True
output_attentions False
tune_start_layer 12
use_adapter true
use_language_emb true
num_adapters 1
adapter_size 256
one_hot false
language_emb_size 32
language_emb_dropout 0.1
language_drop_rate 0.2
num_language_features 289
nl_project 289
language_features syntax_knn+phonology_knn+inventory_knn
adapter_initializer_range 0.0001
in_langs en, de
out_langs 
letter_codes data/letter_codes.json
model BiLSTMCRFModel
lstm_layers 3
word_dims 100
predict_dims 100
dropout_emb 0.0
lstm_hiddens 200
dropout_lstm_input 0.33
dropout_lstm_hidden 0.33
hidden_dims 100
inner_hidden_dims 400
number_heads 8
num_layers 6
dropout_hidden 0.33
learning_rate 2e-3
decay .75
decay_steps 5000
beta_1 .9
beta_2 .9
epsilon 1e-12
clip 1.0
parser_tune 1
train_epochs 40
train_batch_size 32
test_batch_size 32
validate_every 180
update_every 180
save_after 2
Total num:  3687
Total num:  2006
_id2label:  ['<pad>', 'O', 'I-TARGET', 'I-AGENT', 'B-TARGET', 'B-AGENT']
Vocab info: #words 11606, #labels 6

GPU using status:  True
200 1068
Orthogonal pretrainer loss: 7.28e-30
200 600
Orthogonal pretrainer loss: 1.59e-30
200 600
Orthogonal pretrainer loss: 1.41e-30
Load bert vocabulary finished
Token indices sequence length is longer than the specified maximum sequence length for this model (1099 > 512). Running this sequence through the model will result in indexing errors
Total num:  3687
Total num:  2006
Total num:  222
Total num:  2417
Finish code test!
Epoch: 0
Iteration:  1
loss:  0.010404200293123722
Step 0, Epoch 0, 0/178| acc: 7.73| ppl: 6.51| 1074.4 tgt tok/s| 0.80 s elapsed
Iteration:  2
loss:  0.007071651518344879
Step 0, Epoch 0, 1/178| acc: 45.13| ppl: 4.74| 1439.3 tgt tok/s| 1.25 s elapsed
Iteration:  3
loss:  0.007469274569302797
Step 0, Epoch 0, 2/178| acc: 54.30| ppl: 4.41| 1609.5 tgt tok/s| 1.69 s elapsed
Iteration:  4
loss:  0.0031850074883550406
Step 0, Epoch 0, 3/178| acc: 71.59| ppl: 2.85| 1125.6 tgt tok/s| 4.68 s elapsed
Iteration:  5
loss:  0.005116723012179136
Step 0, Epoch 0, 4/178| acc: 72.85| ppl: 2.80| 1176.5 tgt tok/s| 5.15 s elapsed
Iteration:  6
loss:  0.005042515695095062
Step 0, Epoch 0, 5/178| acc: 73.79| ppl: 2.75| 1260.6 tgt tok/s| 5.54 s elapsed
Iteration:  7
loss:  0.005745882634073496
Step 0, Epoch 0, 6/178| acc: 73.74| ppl: 2.76| 1308.4 tgt tok/s| 6.00 s elapsed
Iteration:  8
loss:  0.00513035524636507
Step 0, Epoch 0, 7/178| acc: 74.20| ppl: 2.73| 1354.7 tgt tok/s| 6.65 s elapsed
Iteration:  9
loss:  0.005123358219861984
Step 0, Epoch 0, 8/178| acc: 74.67| ppl: 2.70| 1415.2 tgt tok/s| 7.07 s elapsed
Iteration:  10
loss:  0.005119794514030218
Step 0, Epoch 0, 9/178| acc: 74.90| ppl: 2.69| 1451.3 tgt tok/s| 7.51 s elapsed
Iteration:  11
loss:  0.005895818118005991
Step 0, Epoch 0, 10/178| acc: 74.65| ppl: 2.70| 1489.3 tgt tok/s| 7.96 s elapsed
Iteration:  12
loss:  0.004193409346044064
Step 0, Epoch 0, 11/178| acc: 75.40| ppl: 2.63| 1268.4 tgt tok/s| 10.62 s elapsed
Iteration:  13
loss:  0.0034494593273848295
Step 0, Epoch 0, 12/178| acc: 76.77| ppl: 2.55| 1121.0 tgt tok/s| 13.28 s elapsed
Iteration:  14
loss:  0.005719917360693216
Step 0, Epoch 0, 13/178| acc: 76.53| ppl: 2.56| 1145.3 tgt tok/s| 13.70 s elapsed
Iteration:  15
loss:  0.004532798193395138
Step 0, Epoch 0, 14/178| acc: 76.78| ppl: 2.54| 1186.6 tgt tok/s| 14.21 s elapsed
Iteration:  16
loss:  0.003970799967646599
Step 0, Epoch 0, 15/178| acc: 77.30| ppl: 2.49| 1095.3 tgt tok/s| 16.83 s elapsed
Iteration:  17
loss:  0.004825144074857235
Step 0, Epoch 0, 16/178| acc: 77.41| ppl: 2.49| 1118.5 tgt tok/s| 17.26 s elapsed
Iteration:  18
loss:  0.005862945690751076
Step 0, Epoch 0, 17/178| acc: 77.16| ppl: 2.51| 1145.1 tgt tok/s| 17.68 s elapsed
Iteration:  19
loss:  0.004806846845895052
Step 0, Epoch 0, 18/178| acc: 77.38| ppl: 2.50| 1162.1 tgt tok/s| 18.12 s elapsed
Iteration:  20
loss:  0.0052368249744176865
Step 0, Epoch 0, 19/178| acc: 77.30| ppl: 2.50| 1191.5 tgt tok/s| 18.47 s elapsed
Iteration:  21
loss:  0.004356563091278076
Step 0, Epoch 0, 20/178| acc: 77.72| ppl: 2.48| 1113.7 tgt tok/s| 21.08 s elapsed
Iteration:  22
loss:  0.0042963228188455105
Step 0, Epoch 0, 21/178| acc: 78.02| ppl: 2.47| 1123.7 tgt tok/s| 21.62 s elapsed
Iteration:  23
loss:  0.005590901710093021
Step 0, Epoch 0, 22/178| acc: 77.87| ppl: 2.48| 1139.2 tgt tok/s| 22.00 s elapsed
Iteration:  24
loss:  0.005203734152019024
Step 0, Epoch 0, 23/178| acc: 77.82| ppl: 2.48| 1150.7 tgt tok/s| 22.61 s elapsed
Iteration:  25
loss:  0.005256033502519131
Step 0, Epoch 0, 24/178| acc: 77.75| ppl: 2.48| 1165.2 tgt tok/s| 23.11 s elapsed
Iteration:  26
loss:  0.003989575430750847
Step 0, Epoch 0, 25/178| acc: 77.95| ppl: 2.46| 1092.9 tgt tok/s| 25.92 s elapsed
Iteration:  27
loss:  0.004403877072036266
Step 0, Epoch 0, 26/178| acc: 78.12| ppl: 2.45| 1106.5 tgt tok/s| 26.36 s elapsed
Iteration:  28
loss:  0.00509555684402585
Step 0, Epoch 0, 27/178| acc: 78.10| ppl: 2.46| 1122.7 tgt tok/s| 26.81 s elapsed
Iteration:  29
loss:  0.004192567430436611
Step 0, Epoch 0, 28/178| acc: 78.33| ppl: 2.45| 1140.7 tgt tok/s| 27.20 s elapsed
Iteration:  30
loss:  0.0049429237842559814
Step 0, Epoch 0, 29/178| acc: 78.38| ppl: 2.44| 1157.1 tgt tok/s| 27.54 s elapsed
Iteration:  31
loss:  0.003332737134769559
Step 0, Epoch 0, 30/178| acc: 78.74| ppl: 2.41| 1110.8 tgt tok/s| 30.14 s elapsed
Iteration:  32
loss:  0.005108675919473171
Step 0, Epoch 0, 31/178| acc: 78.71| ppl: 2.42| 1119.7 tgt tok/s| 30.54 s elapsed
Iteration:  33
loss:  0.004619722254574299
Step 0, Epoch 0, 32/178| acc: 78.70| ppl: 2.41| 1137.2 tgt tok/s| 30.97 s elapsed
Iteration:  34
loss:  0.0049278694204986095
Step 0, Epoch 0, 33/178| acc: 78.59| ppl: 2.41| 1151.4 tgt tok/s| 31.35 s elapsed
Iteration:  35
loss:  0.0053094010800123215
Step 0, Epoch 0, 34/178| acc: 78.39| ppl: 2.42| 1165.5 tgt tok/s| 31.76 s elapsed
Iteration:  36
loss:  0.005238787736743689
Step 0, Epoch 0, 35/178| acc: 78.30| ppl: 2.42| 1180.8 tgt tok/s| 32.14 s elapsed
Iteration:  37
loss:  0.004996634554117918
Step 0, Epoch 0, 36/178| acc: 78.35| ppl: 2.42| 1190.0 tgt tok/s| 32.67 s elapsed
Iteration:  38
loss:  0.005122991744428873
Step 0, Epoch 0, 37/178| acc: 78.34| ppl: 2.42| 1194.8 tgt tok/s| 33.51 s elapsed
Iteration:  39
loss:  0.005496697966009378
Step 0, Epoch 0, 38/178| acc: 78.15| ppl: 2.43| 1209.2 tgt tok/s| 33.90 s elapsed
Iteration:  40
loss:  0.004982600919902325
Step 0, Epoch 0, 39/178| acc: 78.05| ppl: 2.43| 1223.9 tgt tok/s| 34.28 s elapsed
Iteration:  41
loss:  0.004314185120165348
Step 0, Epoch 0, 40/178| acc: 78.17| ppl: 2.42| 1240.4 tgt tok/s| 34.69 s elapsed
Iteration:  42
loss:  0.0038808584213256836
Step 0, Epoch 0, 41/178| acc: 78.31| ppl: 2.41| 1254.3 tgt tok/s| 35.10 s elapsed
Iteration:  43
loss:  0.005173870362341404
Step 0, Epoch 0, 42/178| acc: 78.24| ppl: 2.42| 1264.3 tgt tok/s| 35.51 s elapsed
Iteration:  44
loss:  0.004720872268080711
Step 0, Epoch 0, 43/178| acc: 78.21| ppl: 2.41| 1275.7 tgt tok/s| 35.94 s elapsed
Iteration:  45
loss:  0.005494778044521809
Step 0, Epoch 0, 44/178| acc: 78.06| ppl: 2.42| 1287.3 tgt tok/s| 36.32 s elapsed
Iteration:  46
loss:  0.005472436081618071
Step 0, Epoch 0, 45/178| acc: 78.01| ppl: 2.42| 1300.4 tgt tok/s| 36.63 s elapsed
Iteration:  47
loss:  0.0032778193708509207
Step 0, Epoch 0, 46/178| acc: 78.29| ppl: 2.40| 1253.0 tgt tok/s| 39.26 s elapsed
Iteration:  48
loss:  0.00514147337526083
